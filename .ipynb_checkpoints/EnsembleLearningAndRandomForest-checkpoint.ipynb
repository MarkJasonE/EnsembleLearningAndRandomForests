{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#To make the notebook's output stabel across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "#Uses Jupyter's own backend to plot\n",
    "%matplotlib inline\n",
    "\n",
    "#To make pretty figures\n",
    "mpl.rc(\"axes\", labelsize=14)\n",
    "mpl.rc(\"xtick\", labelsize=12)\n",
    "mpl.rc(\"ytick\", labelsize=12)\n",
    "\n",
    "#Path to saving images\n",
    "IMAGE_PATH = os.path.join(\"images\")\n",
    "os.makedirs(IMAGE_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGE_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension ,dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ensemble Learning*** is a technique where you take a group of predictors (such as classifiers and regressors) and you aggregate their predictions, which is most often better than an individual predictor.\n",
    "\n",
    "A group of predictors is called an ***ensemble***<br>\n",
    "An Ensemble Learning algorithm is called an ***Ensemble method***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of **Ensemble method**: train a group of Decision Tree classifiers, each on a different random subset of the training set. To make the predictions, you obtain the predictions of all the individual trees, then predict the class that gets the most votes. Such an ensemble of Decision Trees is called a ***Random Forest***, as simply as it may sound, it's one of the most powerful Machine Learning algorithms available today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When should I use an Esemble method?**\n",
    "\n",
    "It's often used near the end of a project, once a few good predictors have been built, combine them into an even better predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will cover the most popular **Ensemble methods**, _bagging_, _boosting_ and _stacking_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section: Voting Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hard voting*** is when you aggregate the predictions of several classifiers and predict the class that get the most votes (a majority vote classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This voting classifier often achieves a higher accuracy than the best calssifier in the ensemble. Even if each classifier is a ***weak learner*** (it does slightly better than random guessing), the ensemble can still be a ***strong learner*** (achieving high accuracy), provided there are a sufficent number of weak learners and they're sufficiently diverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensemble methods**  work best when the predictors are as independent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms. This increases the chance that they will make very different types of errors, improving the ensemble's accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create and train a voting classifier (**hard voting**), composed of three diverse classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Ensemble and RF - Hard voting classifier predictions.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf_hardv = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "rnd_clf_hardv = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf_hardv = SVC(gamma=\"scale\", random_state=42)\n",
    "\n",
    "voting_clf_hardv = VotingClassifier(\n",
    "    estimators = [(\"lr\", log_clf_hardv), (\"rf\", rnd_clf_hardv), \n",
    "                  (\"svc\", svm_clf_hardv)],\n",
    "    voting = \"hard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(random_state=42)),\n",
       "                             ('rf', RandomForestClassifier(random_state=42)),\n",
       "                             ('svc', SVC(random_state=42))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf_hardv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.85\n",
      "RandomForestClassifier 0.88\n",
      "SVC 0.87\n",
      "VotingClassifier 0.87\n"
     ]
    }
   ],
   "source": [
    "for clf in (log_clf_hardv, rnd_clf_hardv, svm_clf_hardv, voting_clf_hardv):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Soft voting*** is when you predict the class with the highest class probabilty, averaged over all the individual classifiers. Assuming that all classifiers are able to estimate class probablities - they have the _predict_proba()_ method\n",
    "\n",
    "It often achieves a higher performance than hard voting beause it gives more weight to highly confident votes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create and train a voting classifier(**soft voting**), composed of three diverse classifiers\n",
    "\n",
    "**Note** SVC doesn't have the _predict_proba()_ method by default, so we need to set _probability=True_ hyperparameter (this makes SVC use cross validation to estimate class probabilities, slowing down training), then it will add a _predict proba()_ method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(random_state=42)),\n",
       "                             ('rf', RandomForestClassifier(random_state=42)),\n",
       "                             ('svm', SVC(probability=True, random_state=42))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf_softv = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "rnd_clf_softv = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf_softv = SVC(gamma=\"scale\", probability=True, random_state=42)\n",
    "\n",
    "voting_clf_softv = VotingClassifier(\n",
    "    estimators = [(\"lr\", log_clf_softv), (\"rf\", rnd_clf_softv), \n",
    "                  (\"svm\", svm_clf_softv)],\n",
    "    voting = \"soft\"\n",
    ")\n",
    "voting_clf_softv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.85\n",
      "RandomForestClassifier 0.88\n",
      "SVC 0.87\n",
      "VotingClassifier 0.89\n"
     ]
    }
   ],
   "source": [
    "for clf in (log_clf_softv, rnd_clf_softv, svm_clf_softv, voting_clf_softv):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see it achieved a higher accuracy, 89% (87% for **hard voting**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Of Section: Voting Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section: Bagging And Pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've just seen that in order to get a diverse set of classifiers, we need different algorithms. Another approach is to use the same training algorithm for every predictor (e.g classifier) and train them on different random subsets of the training set\n",
    "\n",
    "When sampling is performed ***with replacement***, it's ***called bagging*** (short for bootstrap aggregating). When sampling is performed ***without replacement***, it's ***called pasting***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, both bagging and pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Ensemble and RF - Bagging and pasting on different random samples.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HO_EnsembleAndRandomForest",
   "language": "python",
   "name": "ho_ensembleandrfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
